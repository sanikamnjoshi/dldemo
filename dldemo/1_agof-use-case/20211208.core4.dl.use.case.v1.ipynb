{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agof Use Case\n",
    "\n",
    "In this use case, we will fetch, process and extract some preliminary results from the 'daily digital facts' market media study by agof (Arbeitsgemeinschaft Online Forschung).\n",
    "\n",
    "This notebook is divided into three parts:\n",
    "1. Downloading Raw Data\n",
    "2. Processing Downloaded Data\n",
    "3. Extracting Facts\n",
    "\n",
    "You are encouraged to play around. The blocks with comments beginning in `# note: ...` or `# sanity check: ...` have been included to further your understanding of the logic and to demonstrate how a data scientist will go about working on this problem. These blocks are not vital to the core logic of this use case and can therefore be skipped when you translate the code from this notebook to core4 jobs.\n",
    "\n",
    "Lines of code which produce a large output or which have been commented out with three hash signs (`###`) for better readability. You can uncomment them in order to run them.\n",
    "\n",
    "**A note on python notebooks:** Python notebooks such as this one are flexible in that you can run your code bit by bit. The downside is that you have the possibility to run blocks of code in an arbitrary order. We have divided the code in this notebook into several blocks to explain it more easily. Please make sure you run them in the order in which we have already written them. This also applies when you translate this code into core4 jobs as a part of your assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Downloading Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fetching the webpage\n",
    "\n",
    "import requests\n",
    "\n",
    "url = \"https://www.agof.de/service-downloads/downloadcenter/download-daily-digital-facts/\"\n",
    "rv = requests.get(url)\n",
    "body = rv.content.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: how is the content we fetched stored by python?\n",
    "\n",
    "type(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: have we fetched the right thing?\n",
    "\n",
    "### body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# scraping the fetched content\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(body, \"html.parser\")\n",
    "tables_list = soup.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: have we scraped correctly? (1/2)\n",
    "\n",
    "tables_list[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: have we scraped correctly? (2/2)\n",
    "\n",
    "tables_list[1].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# isolating relevant links from the list of scraped html table rows (<tr>...</tr>)\n",
    "\n",
    "links = [item for item in tables_list if \"Angebote Ranking\" in item.text]\n",
    "links_list = [item for item in links if \"xlsx\" in item.text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: did we isolate the right links?\n",
    "\n",
    "str(links_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# using regular expresssions to extract the link from each string in the list\n",
    "\n",
    "import re\n",
    "re.findall(\"href=[\\\"\\'](.+?)[\\\"\\']\", str(links_list[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xls = []\n",
    "for i in links_list:\n",
    "    xls.append(re.findall(\"href=[\\\"\\'](.+?)[\\\"\\']\", str(i))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: does our list of links look right? (1/3)\n",
    "\n",
    "### xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: does our list of links look right? (2/3)\n",
    "\n",
    "xls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: does our list of links look right? (3/3)\n",
    "\n",
    "len(xls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Processing Downloaded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (1/7)\n",
    "\n",
    "rv = requests.get(xls[0])\n",
    "open(\"/tmp/test.xlsx\", \"wb\").write(rv.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (2/7)\n",
    "\n",
    "# read data without skipping rows and find out where the actual data starts (the first few rows can be metadata)\n",
    "df = pd.read_excel(\"/tmp/test.xlsx\", header=None)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note:\n",
    "# it also possible to read the excel such that we skip the metadata from the very beginning\n",
    "# but in our case, are interested in saving the metadata, so we will NOT be doing this\n",
    "\n",
    "# if you are curious, this is how we would have skipped the rows containing metadata:\n",
    "df_no_metadata = pd.read_excel(\"/tmp/test.xlsx\", skiprows = 8)\n",
    "### df_no_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (3/7)\n",
    "\n",
    "# save metadata in separate variables\n",
    "# in python, \"assert\" raises an error if the condition you pass it is false\n",
    "assert df.iloc[0, 0] == \"Analyse\"\n",
    "analyse = df.iloc[0, 1]\n",
    "assert df.iloc[1, 0] == \"Grundgesamtheit\"\n",
    "grundgesamtheit = df.iloc[1, 1]\n",
    "assert df.iloc[2, 0] == \"Zeitraum\"\n",
    "zeitraum = df.iloc[2, 1]\n",
    "assert df.iloc[3, 0] == \"Vorfilter\"\n",
    "vorfilter = df.iloc[3, 1]\n",
    "vorfilter_fallzahl = df.iloc[4, 1]\n",
    "assert df.iloc[5, 0] == \"Zielgruppe\"\n",
    "zielgruppe = df.iloc[5, 1]\n",
    "zielgruppe_fallzahl = df.iloc[6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (4/7)\n",
    "\n",
    "# identify where the data starts\n",
    "ln = 7\n",
    "while df.iloc[ln, 0] != \"Basis\":\n",
    "    ln += 1\n",
    "    if ln > 1000:\n",
    "        raise  RuntimeError(\"failed to identify start of data\")\n",
    "        \n",
    "ln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (5/7)\n",
    "\n",
    "# save the subset of 'df' containing the main data in a separeate dataframe, 'dframe'\n",
    "dframe = df.iloc[ln:].copy()\n",
    "\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (6/7)\n",
    "\n",
    "# extract column names from 'df' and save them as a list 'cols'\n",
    "cols = list(df.iloc[ln-1])\n",
    "cols[0] = \"Titel\"\n",
    "\n",
    "# name the columns of df using the list 'cols'\n",
    "dframe.columns = [\"\" if pd.isnull(c)\n",
    "             else c.replace(\"\\n\", \" \").replace(\".\", \"\") for c in cols]\n",
    "if \"\" in dframe.columns:\n",
    "    dframe.drop([\"\"], axis=1, inplace=True)\n",
    "\n",
    "# add columns containing the medtadata we saved earlier\n",
    "dframe[\"Analyse\"] = analyse\n",
    "dframe[\"Grundgesamtheit\"] = grundgesamtheit\n",
    "dframe[\"Zeitraum\"] = zeitraum\n",
    "dframe[\"Vorfilter\"] = vorfilter\n",
    "dframe[\"Zielgruppe\"] = zielgruppe\n",
    "\n",
    "dframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: process just the first excel to begin with (7/7)\n",
    "\n",
    "# check how many different time periods ('Zeitraum') are covered in 'dframe'\n",
    "dframe.Zeitraum.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a function that generalizes what we did above with one excel\n",
    "# this function can later be called for any of the excels in 'xls'\n",
    "\n",
    "def process(df):\n",
    "    assert df.iloc[0, 0] == \"Analyse\"\n",
    "    analyse = df.iloc[0, 1]\n",
    "    assert df.iloc[1, 0] == \"Grundgesamtheit\"\n",
    "    grundgesamtheit = df.iloc[1, 1]\n",
    "    assert df.iloc[2, 0] == \"Zeitraum\"\n",
    "    zeitraum = df.iloc[2, 1]\n",
    "    assert df.iloc[3, 0] == \"Vorfilter\"\n",
    "    vorfilter = df.iloc[3, 1]\n",
    "    vorfilter_fallzahl = df.iloc[4, 1]\n",
    "    assert df.iloc[5, 0] == \"Zielgruppe\"\n",
    "    zielgruppe = df.iloc[5, 1]\n",
    "    zielgruppe_fallzahl = df.iloc[6, 1]\n",
    "    ln = 7\n",
    "    while df.iloc[ln, 0] != \"Basis\":\n",
    "        ln += 1\n",
    "        if ln > 1000:\n",
    "            raise  RuntimeError(\"failed to identify start of data\")\n",
    "    d = df.iloc[ln:].copy()\n",
    "    cols = list(df.iloc[ln-1])\n",
    "    cols[0] = \"Titel\"\n",
    "    d.columns = [\"\" if pd.isnull(c)\n",
    "                 else c.replace(\"\\n\", \" \").replace(\".\", \"\") for c in cols]\n",
    "    if \"\" in d.columns:\n",
    "        d.drop([\"\"], axis=1, inplace=True)\n",
    "    d[\"Analyse\"] = analyse\n",
    "    d[\"Grundgesamtheit\"] = grundgesamtheit\n",
    "    d[\"Zeitraum\"] = zeitraum\n",
    "    d[\"Vorfilter\"] = vorfilter\n",
    "    d[\"Zielgruppe\"] = zielgruppe\n",
    "    return(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a list to store the data generated after processing excel files\n",
    "# save processed data from xls[0] in it\n",
    "fin_df = list()\n",
    "fin_df.append(dframe)\n",
    "\n",
    "# process several excels from 'xls' using the 'process()' function we defined above\n",
    "# we process 29 files in this case (range(1,30))\n",
    "# we start 'i' from 1 as we have already added processed data from xls[0] to the list 'fin_df'\n",
    "for i in range(1,30):\n",
    "    rv = requests.get(xls[i])\n",
    "    open(\"/tmp/test\"+str(i)+\".xlsx\", \"wb\").write(rv.content)\n",
    "    df = pd.read_excel(\"/tmp/test\"+str(i)+\".xlsx\", header=None)\n",
    "    df_processed = process(df)\n",
    "    fin_df.append(df_processed)\n",
    "\n",
    "# saving 'fin_df' as a dataframe    \n",
    "fin_df = pd.concat(fin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: what does 'fin_df' look like now? (1/2)\n",
    "\n",
    "fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: what does 'fin_df' look like now? (2/2)\n",
    "\n",
    "fin_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: how many different time periods ()'Zeitraum') are covered in 'fin_df'\n",
    "\n",
    "fin_df.Zeitraum.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "MONAT = {\n",
    "    \"Januar\": \"01\",\n",
    "    \"Februar\": \"02\",\n",
    "    \"MÃ¤rz\": \"03\",\n",
    "    \"April\": \"04\",\n",
    "    \"Mai\": \"05\",\n",
    "    \"Juni\": \"06\",\n",
    "    \"Juli\": \"07\",\n",
    "    \"August\": \"08\",\n",
    "    \"September\": \"09\",\n",
    "    \"Oktober\": \"10\",\n",
    "    \"November\": \"11\",\n",
    "    \"Dezember\": \"12\"\n",
    "}\n",
    "\n",
    "monat = fin_df.Zeitraum.apply(lambda s: s.replace(\"Letzter Monat (\", \"\").replace(\")\", \"\").split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: what data type is 'monat' stored as?\n",
    "\n",
    "type(monat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: what does 'monat' look like?\n",
    "\n",
    "monat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# adding columns to 'fin_df'\n",
    "\n",
    "fin_df[\"Monat\"] = [datetime.datetime.strptime(\"01.\" + MONAT[m[0]] + \".\" + m[1], \"%d.%m.%Y\") for m in monat]\n",
    "fin_df[\"val\"] = fin_df[\"Kontakte Mio\"].apply(pd.to_numeric, errors='coerce')\n",
    "fin_df['Date'] = fin_df.Monat.apply(lambda x: x.date().isoformat())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: list the columns in 'fin_df'\n",
    "\n",
    "fin_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check: check the values different categorical variables take on (here, the variable 'Medientyp')\n",
    "\n",
    "fin_df.Medientyp.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "fin_df = fin_df.replace(np.nan,0)\n",
    "g = fin_df.groupby([\"Date\"]).val.sum()\n",
    "g.plot.bar()\n",
    "plt.ylabel(\"Contacts\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "df_new = fin_df[fin_df.Medientyp != 0]\n",
    "g1 = df_new.groupby([\"Medientyp\"]).val.sum()\n",
    "g1.plot.bar()\n",
    "plt.ylabel(\"Contacts\")\n",
    "plt.xticks(rotation='horizontal')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "list(g1.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "df_new = fin_df[fin_df.Medientyp != 0]\n",
    "# Monthly contacts for each media group\n",
    "g1 = df_new.groupby([\"Date\",\"Medientyp\"]).val.sum().unstack()\n",
    "# contact of different media group per month\n",
    "# g1 = df_new.groupby([\"Date\",\"Medientyp\"]).val.sum().unstack(0)\n",
    "plt.rcParams[\"figure.figsize\"] = [7,7]\n",
    "g1.plot.bar(rot=45)\n",
    "\n",
    "plt.ylabel(\"Contacts\")\n",
    "plt.legend(fontsize='small')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
